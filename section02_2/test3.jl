{"contents": "Web scraping is when you extract data from the web and put it in a structured format. Getting structured data from publicly available websites and pages should not be an issue as everybody with an internet connection can access these websites. You should be able to structure it as well. In reality though, it\u2019s not that easy. One of the main use cases of web scraping is in the e-commerce world: price monitoring and  . However, you can also use web scraping for  ,  , "}
{"contents": "As a python developer at Scrapinghub, I spend a lot of time in the Scrapy shell. This is a command-line interface that comes with Scrapy and allows you to run simple, spider compatible code. It gets the job done, sure, but there\u2019s a point where a command-line interface can become kinda fiddly and I found I was passing that point pretty regularly. I have some background in tool design and task automation so I naturally began to wonder how I could improve my experience and help focus more time on building great spiders. Over my Christmas break, I dedicate some free time to improve this experience, resulting in my first python package  . Scrapy-GUI offers two different UI tools to help build Scrapy spiders, but today I am going to focus on the part that integrates directly into a Scrapy shell - its   method. First things first, the installation part is simple, as it is on the Python Package Index. All you need to do is install it via pip. "}
{"contents": "We\u2019re excited to announce our newest data extraction API,  . From now on, you can use AutoExtract to extract Job Postings data from many job boards and recruitment sites. Without writing any custom data extraction code! Aside from e-commerce products and news extraction, one of the most demanded web data types is job postings. Job Postings API enables you to get real-time job postings data, at scale. If you want to learn more about the activities of fortune 100/500/1000 companies, looking at their job postings could be an essential element of your research. It can indicate what direction the companies are heading and give you outside insights on what technologies they are investing in. Job postings can also be a great addition to your business intelligence (BI) data sources. By monitoring job postings, you can also get a clear understanding of what direction your competitors, partners or suppliers are moving. Getting access to job postings data can help you determine what markets your competitors are going after."}
{"contents": "The Internet offers a  in the form of  , news, blog posts, stories, essays, tutorials that can be leveraged by many useful applications: But anyone interested in using all this data available, will face some challenges.\u00a0 Web pages are built of many components (menus, sidebars, ads, etc) and only a few of them represent the true article content, the actual valuable information. Being able to "}
{"contents": "Scrapinghub is a fully distributed organization with a remote workforce spread across the globe. This structure will enable us to continue to operate at full capacity during the Coronavirus pandemic and deliver full service to our customers. Businesses all over the world are trying to adapt to the new circumstances brought on by the Coronavirus such as being forced to implement a remote working environment while retaining productivity, a huge challenge for companies that normally don\u2019t work remotely. We have had a lot of queries from our customers, who are doing internal global risk assessments on their supply chains being affected by COVID-19 so want to share our continued commitment to providing our customers with ongoing services during this time while ensuring a safe environment for all of our employees. Our internal risk models predict no more than 2% leave as a worst-case scenario, only slightly above baseline. This is likely to be offset by people deferring normal vacation leave due to travel limitations. Scrapinghub is at exceptionally low risk from any disruption in supplying our customers from the pandemic for two reasons:"}
{"contents": "When it comes to web scraping at scale, there\u2019s a set of challenges you need to overcome to extract the data. But once you are able to get it, you still have work to do. You need to have a data QA process in place. Data quality becomes especially crucial if you\u2019re extracting high volumes of data from the web regularly and your team\u2019s success depends on the quality of the scraped data. \n \n This article is the first of a four-part series of how to maximize web scraped data quality. We are going to share with you all the techniques, tricks and technologies we use at Scrapinghub to extract web data from billions of pages every month, while keeping data quality high. The first step is to understand the business requirements of the web scraping project and define clear, testable rules which will help you detect data quality problems. Understanding requirements clearly is essential to move forward and develop the best data quality process."}
{"contents": "I\u2019d like to echo Joel Gasgoine\u2019s sentiments: This is not normal remote working! Like Buffer, we\u2019ve been a remote-first company for almost 10 years and we\u2019re also adjusting to the new normal as a result of COVID-19. Remote teams tend to favour asynchronous written communication. That is to say, we send text messages and don\u2019t expect an immediate response. This allows longer blocks of uninterrupted working time, it allows \u201cbatching\u201d of replies and works better when people have different working hours. Amir Salihefendic makes the case for Asynchronous Communication in his excellent blog post  . This is one of the reasons why so many remote workers feel more productive. On the other hand, synchronous communication is more effective when a topic requires a lot of back-and-forths, or if there is time pressure. Don\u2019t be afraid to \u201cjump on a call\u201d to quickly sort things out. Meetings are better for brainstorming or to achieve a consensus, however, this can be more difficult remotely, especially if participants are not used to it."}
{"contents": "When you extract data from the web at scale, quality assurance is an important process to make sure your web extracted data is consistently of high quality. Validation of this data can be complex though. There are many challenges and problems that need to be addressed. In the second part of this series on web data quality assurance, we will cover most common hurdles and pitfalls in data validation and how to deal with them. I still remember my first lesson when I joined the team. My manager shared 3 simple questions to keep in our mind working on data validation: The problems will be listed in their natural appearance in a typical web scraping project. In the previous post, we discussed the importance of clear, testable requirements. Now let's add more details about what else could be challenging at this point. The QA department is responsible for defining good tests, both in terms of "}
{"contents": "Today we are delighted to launch a Beta of our newest data extraction API:  . With this API you can collect structured data from web pages that contain automotive data such as classified or dealership sites. Using our API, you can get your data without writing site-specific code. If you need automotive/vehicle data, sign up now for a beta version of our Vehicle API. Whether you are interested in car prices, VIN or other car specific details, our Vehicle API can extract that data for you, at scale. With  , you can get access to all the publicly visible details and technical information about the vehicle in a structured JSON."}
{"contents": "Web scraping projects usually involve data extraction from many websites. The standard approach to tackle this problem is to write some code to navigate and extract the data from each website. However, this approach may not scale so nicely in the long-term, requiring maintenance effort for each website; it also doesn\u2019t scale in the short-term, when we need to start the extraction process in a couple of weeks. Therefore, we need to think of different solutions to tackle these issues. The problem we propose to solve here is related to   that can be available in HTML form or files, such as PDFs. The catch is that this is required for a few hundreds of different domains and we should be able to scale it up and down without much effort. A brief outline of the problem that needs to be solved: In terms of the solution, file downloading is already built-in Scrapy, it\u2019s just a matter of finding the proper URLs to be downloaded. A routine for HTML article extraction is a bit more tricky, so for this one, we\u2019ll go with AutoExtract\u2019s "}
